include:
  - project: 'a10179/cicd-pipelines'
    file:
      - '/templates-stable/.node-ci-template.yml'

stages:
  - build
  - version-check
  - sonarqube-check
  - package
  - release
  - containerize
  - deploy

.deploy:
  create_tls:
    - export SECRET_NAME="$CI_PROJECT_NAME-tls"
    - apt update && apt install -y openssl
    - |
        if kubectl get secret $SECRET_NAME --namespace=$NAMESPACE &> /dev/null; then
            echo "Secret $SECRET_NAME already exists. Skipping creation."
        else
            echo "Creating secret $SECRET_NAME..."
            openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout $CI_PROJECT_NAME.key -out $CI_PROJECT_NAME.cert -subj "/CN=$HOST/O=blue.ai"
            kubectl create secret tls $SECRET_NAME \
                --cert=$CI_PROJECT_NAME.cert \
                --key=$CI_PROJECT_NAME.key \
                --namespace=$NAMESPACE
        fi
  deploy_to_environment:
    - echo $DOCKER_TAG
    - echo $APP_VERSION
    # Replace variables in values.tmpl with values from variables.env
    - |
      if [ -f ./deployments/"$CI_PROJECT_NAME"/values.yaml ]; then
          envsubst < ./deployments/"$CI_PROJECT_NAME"/values.tmpl > ./deployments/"$CI_PROJECT_NAME"/values.yaml
          helm upgrade -i --dry-run "$CI_PROJECT_NAME" ./deployments/"$CI_PROJECT_NAME" -f ./deployments/"$CI_PROJECT_NAME"/values.yaml -n "$NAMESPACE" || { echo "helm upgrade --dry-run failed"; exit 1;}
          echo "dry run checks out, upgrading..."
          helm upgrade -i "$CI_PROJECT_NAME" ./deployments/"$CI_PROJECT_NAME" -f ./deployments/"$CI_PROJECT_NAME"/values.yaml -n "$NAMESPACE" --version $APP_VERSION --wait --timeout 600s
          helm_upgrade_exit_status=$?
          export helm_upgrade_exit_status
          if [ "$helm_upgrade_exit_status" -eq 0 ]; then
              echo "helm upgrade succeeded"
              exit 0
          else
              echo "helm upgrade failed, exit status: $helm_upgrade_exit_status"
              echo "Rolling back to the previous version..."
              previous_version=$(helm history -n "$NAMESPACE" "$CI_PROJECT_NAME" | tail -n 1 | awk '{print $1}' | head -n 1)
              export previous_versi
              if [ "$previous_version" -eq 0 ]; then
                  echo "no previous version found"
                  exit 1
              else
                  echo "rolling back to version $previous_version"
                  helm rollback -n "$NAMESPACE" "$CI_PROJECT_NAME" "$previous_version" --wait --timeout 600s
                  helm_rollback_exit_status=$?
                  export helm_rollback_exit_status
                  if [ "$helm_rollback_exit_status" -eq 0 ]; then
                      echo "helm rollback succeeded"
                      exit 1
                  else
                      echo "helm rollback failed, exit status: $helm_rollback_exit_status"
                      exit 1
                  fi
              fi
          fi
      elif [ -f ./deployments/delpoyment.tmpl ]; then
        envsubst < ./deployment/deployment.tmpl > ./deployment/deployment.yaml
        kubectl apply -f ./deployment/deployment.yaml
        kubectl rollout restart deployment "${CI_PROJECT_NAME}" -n "$NAMESPACE"
        if kubectl rollout status deployment "${CI_PROJECT_NAME}" -n "$NAMESPACE" --timeout=300s; then
          echo "Deployment succeeded"
          kubectl annotate deployments.apps "${CI_PROJECT_NAME}" -n "$NAMESPACE" kubernetes.io/change-cause="version ${version}" --overwrite=true
          kubectl rollout history deployment "${CI_PROJECT_NAME}" -n "$NAMESPACE" | tail -n 2 | echo "Deployment revision: $(awk '{print $1}' | head -n 1)"
        else
          echo "Deployment failed"
          kubectl rollout undo deployment "${CI_PROJECT_NAME}" -n "$NAMESPACE"
          kubectl rollout history deployment "${CI_PROJECT_NAME}" -n "$NAMESPACE" | tail -n 2 | echo "Deployment revision: $(awk '{print $1}' | head -n 1)"
          exit 1
        fi
      else
          echo "no deployment file found"
          exit 1
      fi

deploy-to-develop:
  stage: deploy
  environment: develop
  # This job needs release_job and build-docker-image to run and succeed
  needs:
    - job: build-docker-image
      artifacts: true
  variables:
    # Set namespace based on project name as a variable
    NAMESPACE: $CI_PROJECT_NAME-develop
    VITE_APP_PREFIX: ${VITE_APP_PREFIX}
    VITE_APP_API_BASE_URL: ${VITE_APP_API_BASE_URL}
  image:
    # Use helm and kubectl image
    name: dtzar/helm-kubectl:3.13
    # Override entrypoint to use bash
    entrypoint: [""]
  rules:
    # This job will only run if branch name is master
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      when: manual
    - if: $CI_COMMIT_BRANCH =~ /DEVO*/
      when: manual  
    - when: never
  before_script:
    # This job needs the before_script from .kube-requirments to run
    - if [ -n "$KUBE_CONTEXT" ]; then kubectl config use-context "$KUBE_CONTEXT"; fi
    # - !reference [.deploy, create_tls]
  script:
    - !reference [.deploy, deploy_to_environment]

deploy-to-qa:
  stage: deploy
  environment: qa
  # This job needs release_job and build-docker-image to run and succeed
  needs:
    - job: build-docker-image
      artifacts: true
  variables:
    # Set namespace based on project name as a variable
    NAMESPACE: $CI_PROJECT_NAME-qa
    VITE_APP_PREFIX: ${VITE_APP_PREFIX}
    VITE_APP_API_BASE_URL: ${VITE_APP_API_BASE_URL}
  image:
    # Use helm and kubectl image
    name: dtzar/helm-kubectl:3.13
    # Override entrypoint to use bash
    entrypoint: [""]
  rules:
    # This job will only run if branch name is master
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      when: manual
    - if: $CI_COMMIT_BRANCH =~ /DEVO*/
      when: manual  
    - when: never
  before_script:
    # This job needs the before_script from .kube-requirments to run
    - if [ -n "$KUBE_CONTEXT" ]; then kubectl config use-context "$KUBE_CONTEXT"; fi
    # - !reference [.deploy, create_tls]
  script:
    - !reference [.deploy, deploy_to_environment]

deploy-to-dev:
  stage: deploy
  environment: dev
  # This job needs release_job and build-docker-image to run and succeed
  needs:
    - job: build-docker-image
      artifacts: true
  variables:
    # Set namespace based on project name as a variable
    NAMESPACE: dev-shour
    VITE_APP_PREFIX: ${VITE_APP_PREFIX}
    VITE_APP_API_BASE_URL: ${VITE_APP_API_BASE_URL}
  image:
    # Use helm and kubectl image
    name: dtzar/helm-kubectl:3.13
    # Override entrypoint to use bash
    entrypoint: [""]
  rules:
    # This job will only run if branch name is master
    - if: $CI_COMMIT_BRANCH =~ /CMS*/
      when: manual
    - if: $CI_COMMIT_BRANCH =~ /DEVO*/
      when: manual  
    - when: never
  before_script:
    # This job needs the before_script from .kube-requirments to run
    - if [ -n "$KUBE_CONTEXT" ]; then kubectl config use-context "$KUBE_CONTEXT"; fi
    # - !reference [.deploy, create_tls]
  script:
    - !reference [.deploy, deploy_to_environment]